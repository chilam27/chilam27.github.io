<!doctype html><html><head><title>Improve Product Review System</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><meta property="og:title" content="Improve Product Review System"><meta property="og:description" content="Improved the traditional product review system through natural language processing and topic modeling."><meta property="og:type" content="article"><meta property="og:url" content="https://hugo-toha.github.io/posts/improve_product_review_system/"><meta property="article:published_time" content="2020-08-28T00:00:00+00:00"><meta property="article:modified_time" content="2020-08-28T00:00:00+00:00"><meta name=description content="Improved the traditional product review system through natural language processing and topic modeling."><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/>Dao Chi's Portfolio</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><a href=/posts/analyze_predict_house_price/analyze_predict_house_price/ title="Analyze and Predict House Prices">Analyze and Predict House Prices</a></li><li><a class=active href=/posts/improve_product_review_system/ title="Improve Product Review System">Improve Product Review System</a></li><li><a href=/posts/pubg_player_role_analysis/ title="PUBG Player Role Analysis">PUBG Player Role Analysis</a></li><li><a href=/posts/stock_price_prediction/ title="Stock Price Prediction">Stock Price Prediction</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://hugo-toha.github.io/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/default-avatar_hu64ccddcebcea17e48c2247350ae0b3d2_162491_120x120_fit_q75_box.jpg alt="Author Image"><h5 class=author-name>Dao Chi Lam</h5><p>August 28, 2020</p></div><div class=title><h1>Improve Product Review System</h1></div><div class=post-content id=post-content><p>Improve the traditional product review system through text sentimental analysis and Natural Language Processing (NLP). By taking only the customer review comment about the product as input, I can create a sentiment detection algorithm to rate the product based on the comment. I will also include topic modeling in this project to detect the general topics of what the reviews are about.</p><h2 id=background-and-motivation>Background and Motivation</h2><p>I have an idea for this project from the situation I had when doing the thing that most college students do when they are bored: online shopping. I am a very frugal buyer myself. I spend a lot of time looking at the product description and its reviews to make sure I will not regret my purchases. Besides giant e-commerce companies (such as Amazon, Etsy, eBay, etc.), not a lot of other online retailers have anything better than a basic star and comment review system for their products. That is why, in my belief, those top-tier companies are having better customer interactions. Better customer interactions can lead to sellers improving their products and buyers, like me, have more confidence in buying the product.</p><p>Combine with a new principle I just learned that is called <em>Principle of Least Effort</em> (it was first articulated by the Italian philosopher Guillaume Ferrero) and the idea that businesses improve by reducing the friction that is needed for a customer to buy their product (I learned it from a book: Atomic Habits by James Clear). I want to build a review system that reduces one step needed in writing a review: rating the product. To build a system like so, I need to have the machine analyze the review comment and build a model that can predict the rating.</p><p>Since this project touch on many of the fields I am unfamiliar with, there are a couple of things that I wanted to learn out of this project that I hope will benefit my journey of becoming a better data scientist:</p><ol><li>Scrape raw reviews from Amazon.</li><li>Learn about text and sentimental analysis.</li><li>Understand how feature engineer can give me a better outcome.</li><li>Classification algorithms and the importance of feature scaling.</li></ol><h2 id=prerequisites>Prerequisites</h2><p>Python Version: 3.7.4</p><p>Packages: BeautifulSoup, requests, nltk, re, matplotlib.pyplot, seaborn, WordCloud, gensim, pandas, numpy, sklearn, itertools</p><h2 id=project-outline>Project Outline</h2><ol><li><p>Data collection: I build a web scrapper through <code>BeautifulSoup</code> and scrape an amazon product&rsquo;s review. For this project, I choose a clothing item: <strong>Dickies Men&rsquo;s Original 874 Work Pant</strong>. The reason for my decision is I have more experience with the product and the quantity for the review is ideal.</p></li><li><p>Text preprocessing: or data cleaning; I mostly cleaned the review texts to remove noise and make it easy for the machine to read in the data.</p></li><li><p>Exploratory Data Analysis: I analyze the target variable (&ldquo;rating&rdquo;) and examine its and other features' relationships. In this phase, I also perform Latent Dirichlet Allocation (LDA) topic modeling to search for topics of each rating category.</p></li><li><p>Model Building: I compare different classification algorithms (logistic regression, Naive Bayes, random forest classifier, k-nearest neighbor (KNN), and support vector machines (SVM)) and choose the one that produces the best result. The performance metric I use for my multilabel classification algorithm is the accuracy classification score that computes subset accuracy.</p></li></ol><h3 id=data-collection>Data Collection</h3><p><em>*Disclaimer: this data set is used for educational purposes.</em></p><p>I create a web scraper for product reviews. My first intention was to scrape 8 different variables (see table below) plus the product&rsquo;s size and color. I want to see if it has any effect on determining the rating or not (such as whether such size or color has some defects). But I run into a similar issue I have with my other web scrapper: that is I cannot make the function see null value as n/a and it will just skip over. So I decided to leave those two variables out of the data set. Although I want to scrape all 9,208 reviews it has, Amazon only allows me to access only 5,000 of the reviews. Hence, my data scrapped CSV consists of 8 variables and 5,000 records.</p><table><thead><tr><th style=text-align:center>Variables</th><th style=text-align:center>Description</th></tr></thead><tbody><tr><td style=text-align:center>customer_id</td><td style=text-align:center>the unique ID of each customer</td></tr><tr><td style=text-align:center>customer_name</td><td style=text-align:center>name of the customer</td></tr><tr><td style=text-align:center>rating</td><td style=text-align:center>customer&rsquo;s rating of the product (1-5)</td></tr><tr><td style=text-align:center>review_date</td><td style=text-align:center>review&rsquo;s posted date</td></tr><tr><td style=text-align:center>review_loc</td><td style=text-align:center>customer&rsquo;s location (only take reviews from the United States)</td></tr><tr><td style=text-align:center>verified_purchase</td><td style=text-align:center>customer&rsquo;s product purchased verification</td></tr><tr><td style=text-align:center>review_head</td><td style=text-align:center>review&rsquo;s title</td></tr><tr><td style=text-align:center>review_body</td><td style=text-align:center>the main part of the review</td></tr></tbody></table><h3 id=text-preprocessing>Text Preprocessing</h3><ul><li>By glancing at the data set: according to figure 1, only one of our data is numerical (&ldquo;rating&rdquo;) and the rest is categorical. Another important element is to determine whether there is any null value. Luckily for me, there is none.</li></ul><p><img src=/posts/images/improve_product_review_system/fig1.png alt="Figure 1" class=center><div style=margin-top:rem></div></p><ul><li><p>I combine both the review&rsquo;s header and body for easier analysis later on. And since both &ldquo;verified_purchase&rdquo; and &ldquo;review_loc&rdquo; only have one unique value for each variable, I remove them from the data set because it will not give us any information.</p></li><li><p>Generalize &ldquo;review_date&rdquo;: To see the relationship between &ldquo;rating&rdquo; and &ldquo;review_date&rdquo;, I need to group the individual date for a more compact and reasonable graph later on. I want to group them by quarters at first, but I find that I need to compact it even more so I end up doing it by years instead.</p></li><li><p>Then, I create a function (&ldquo;clean_text&rdquo;) to perform text preprocessing to the review texts. I implement the following steps: convert text to lowercase, replace contractions with their longer forms, remove punctuations and numbers, tokenization (a process of splitting strings into tokens), remove stop words, lemmatization(return a word to its common base root while takes into consideration the morphological analysis of the words), only get word that has more than one character. Since there will be some short reviews that end up with empty values after the text is cleaned, I remove them from the data set.</p></li><li><p>I will do a bit of feature engineering and use VADER Sentiment (<em>SentimentIntensityAnalyzer</em>) as a sentiment analysis tool to analyze the emotion of the review text. This tool is very good at not only determine whether a string of text is positive or negative, but it also gives the string a sentiment intensity score. Since the score is ranged from [-1,1], I label the score as follows:</p><ul><li>score >= 0.6: 5</li><li>0.6 > score >= 0.2: 4</li><li>0.2 > score >= -0.2: 3</li><li>-0.2 > score >= -0.4: 2</li><li>-1 >= score: 1</li></ul></li><li><p>Lastly, I will add two more features by determining the string character&rsquo;s length and word count.</p></li></ul><h3 id=eda>EDA</h3><ul><li>Univariate analysis on target variable (&ldquo;rating&rdquo;): I first look at the count of each rating to see if there is an imbalance. From the bar chart in figure 2, ratings from &ldquo;1&rdquo; - &ldquo;4&rdquo; are in a close range (from 400 - 750 counts range), but the rating of &ldquo;5&rdquo; has over 2600 counts. Although it is quite substantial, in terms of business, there are many more customers that are very satisfied with the product than those that are not. We can say that, overall, customers are happy with their purchases.</li></ul><p><img src=/posts/images/improve_product_review_system/fig2.png alt="Figure 2" class=center><div style=margin-top:rem></div></p><ul><li>Multivariate analysis on target variable (&ldquo;rating&rdquo;): next I add time as another element to the analysis. I am wondering whether time has any effect on the customers' rating: maybe the product was bad and people gave negative feedback then the manufacturer improved it and people liked it more or vice versa? Based on the figure below, it turned out that the trend is quite the same. From 2013 to 2017, the rating count is ranked that &ldquo;1&rdquo; has the lowest count, and the higher the rating the higher the count. It is not until 2028 to 2020 that the count for &ldquo;1&rdquo; increases in terms of count and proportion of the rating for the year and is ranked as the second-highest count rank. We do not have enough information to conclude that, in general, customers are not liking the product anymore. But these are very good data for the manufacturer to start taking into consideration.</li></ul><p><img src=/posts/images/improve_product_review_system/fig3.png alt="Figure 3" class=center><div style=margin-top:rem></div></p><ul><li>Next on the list are &ldquo;rating&rdquo; vs. &ldquo;character_len&rdquo; and &ldquo;rating&rdquo; vs. &ldquo;word_count&rdquo;: because both box plots have very similar trends so I will analyze them as one. My initial assumption was that there would be a semi-clear trend of the higher the rating, the fewer words or character length the review has. Although there would be outliers (say a customer loves the product and ending up writing paragraphs about it), my assumption is based on the fact that customer is more likely to criticize more when they are dissatisfied with the purchase. Two graphs below prove my assumption, but the trend is not as clear as I imagine it would be: the &ldquo;5&rdquo; rating&rsquo;s count range and the median is smaller than the others, but the differences are not too significant. Maybe the result would be better if I graph these variables before cleaning the review text.</li></ul><p><img src=/posts/images/improve_product_review_system/fig4_5.png alt="Figure 4 & 5" class=center><div style=margin-top:rem></div></p><ul><li>My last multivariate analysis is to see how accurate is the VADER Sentiment in column &ldquo;predict_sentiment&rdquo; by comparing its and the &ldquo;rating&rdquo; count. Based on the figure below, it does not predict as well as I would hope for. From &ldquo;3&rdquo; to &ldquo;5&rdquo; rating, the differences between the two counts are not as big of a gap. But it does pretty badly when trying to predict the sentiment of the &ldquo;1&rdquo; and &ldquo;2&rdquo; ratings. I can conclude that the VADER Sentiment might be over-rating the sentiment, so I need to expect to see many reviews where they are rated negatively but the &ldquo;predicted_sentiment&rdquo; variable incorrectly states that it is positive.</li></ul><p><img src=/posts/images/improve_product_review_system/fig6.png alt="Figure 6" class=center><div style=margin-top:rem></div></p><ul><li>After analyzing the variables, I want to have a closer look at the cleaned review text itself. I create a word cloud of all the review text for this product in figure 6. This has a very good representation of what words are being repeated the most by having it in different font sizes (the bigger the fonts, the higher the counts). Just by looking at the picture itself, besides the obvious, here are some observations:<ul><li>I can see many text from &ldquo;5&rdquo; star ratings: &ldquo;five star&rdquo;, &ldquo;perfect&rdquo;, &ldquo;buy&rdquo;, &ldquo;great work&rdquo;, etc.</li><li>I can also get a sense of what most of the reviews are about: &ldquo;size&rdquo;, &ldquo;quality&rdquo;, &ldquo;durable&rdquo;, &ldquo;fit&rdquo;.</li><li>Negative review&rsquo;s texts are also present in the word cloud: &ldquo;run small&rdquo;, &ldquo;waist size&rdquo;, &ldquo;tight&rdquo;, &ldquo;return&rdquo;, etc.</li></ul></li></ul><p><img src=/posts/images/improve_product_review_system/fig7.png alt="Figure 7" class=center><div style=margin-top:rem></div></p><ul><li>Word cloud might be a great tool to have a general understanding of the text, but I can analyze it better if I look specifically at the word frequency of each rating. Below are frequency graphs of the top 20 words from the five ratings. Here are some observations:<ul><li>Figure 7a-7c: I grouped these three graphs because they have a similar trend. Just by looking at the top 5 words with the highest frequency (&ldquo;size&rdquo;, &ldquo;pant&rdquo;, &ldquo;small&rdquo;, &ldquo;waist&rdquo;, &ldquo;fit&rdquo;), I can already see that customers are not satisfied with the product is not happy about sizing the most. It is not a surprise to see many people would return the product because the word &ldquo;return&rdquo; is also in the graph.</li><li>Figure 7d: this is where we see the turning point the clearest: this graph has more positive words ranked top of the graph. Interestingly, the word &ldquo;small&rdquo; is still present in the top 10.</li><li>Figure 7e: this graph is filled with only neutral to positive words. But also interesting to note, although reviews do seem to compliment the fitting of the product, they seem to be more positive than previous ratings.</li></ul></li></ul><p><img src=/posts/images/improve_product_review_system/fig8a.png alt="Figure 8a" class=center>
<img src=/posts/images/improve_product_review_system/fig8b.png alt="Figure 8b" class=center>
<img src=/posts/images/improve_product_review_system/fig8c.png alt="Figure 8c" class=center>
<img src=/posts/images/improve_product_review_system/fig8d.png alt="Figure 8d" class=center>
<img src=/posts/images/improve_product_review_system/fig8e.png alt="Figure 8e" class=center><div style=margin-top:rem></div></p><ul><li>For my final analysis, I implement the Latent Dirichlet Allocation (LDA) topic modeling. Although this step may seem repetitive, because the outcome might just be similar to what I have analyzed, I want to see how accurate is this unsupervised learning approach in identifying topics that are being talked about in the reviews' text. It turns out that LDA topic modeling identifies the topics quite well. Below are the result and five examples from each rating:<ul><li>1-3 stars reviews: consist of mostly &ldquo;size&rdquo;, &ldquo;pant&rdquo;, &ldquo;small&rdquo;, &ldquo;waist&rdquo;, and &ldquo;fit&rdquo; words. They indicate that the pants are too small around the waist for those unhappy customers.</li><li>4-5 stars reviews: consist of &ldquo;pant&rdquo;, &ldquo;size&rdquo;, &ldquo;fit&rdquo;, &ldquo;good&rdquo;, and &ldquo;work&rdquo;. They indicate that the pant has a good fit. &ldquo;work&rdquo; here might also means that it is great for them to do their work in since this is a workwear type of pants.</li></ul></li></ul><p><img src=/posts/images/improve_product_review_system/fig9a.png alt="Figure 9a" class=center>
<img src=/posts/images/improve_product_review_system/fig9b.png alt="Figure 9b" class=center><div style=margin-top:rem></div></p><p>This makes me wonder about the product sizing being polarized. Although different people have different shapes and sizes might be a good guess, but it does not align with the number of dissatisfied reviews. So, my assumption has to do with different sizes of the product that might cause these criticisms: that is some product sizes might be scaled disproportionately.</p><h3 id=model-building>Model Building</h3><ul><li><p>Perform vectorization on review texts: the process of converting words into numbers so that the machine can understand. For this project, I use Term Frequency-Inverse Document Frequency model (TFIDF) vectorizer + ngrams (bi-gram) technique. I have tried it with different ngrams (monogram, bi-gram, tri-gram) but it seems that bi-gram help to produce the best result.</p></li><li><p>After removing unnecessary columns (&ldquo;customer_id&rdquo;, &ldquo;customer_name&rdquo;, &ldquo;review_header&rdquo;, &ldquo;review_body&rdquo;, &ldquo;review_txt&rdquo;, &ldquo;review_cleaned&rdquo;, I split the data to training (80%) and testing (20%) sets in a stratify fashion: stratas are the different ratings. The reason for this there is a lot more &ldquo;5&rdquo; stars rating compares to others. By splitting it in stratify fashion, I can eliminate the bias of having an over-whelmed majority of that rating.</p></li><li><p>Because the data has many different ranges of value (such as character_len and those text vectorization columns), I rescale those values into a 0 to 1 range using the function <code>MinMaxScaler</code>. This helps the model produce a more accurate result.</p></li></ul><p><img src=/posts/images/improve_product_review_system/fig10.png alt="Figure 10" class=center><div style=margin-top:rem></div></p><ul><li>The first model I apply for this data set is the <em>Logistic Regression</em>. Although the model is known more for its binary classification, not quite what I want to predict (which is 5 different ratings), I want to apply it in this project to see how it does compare to other more advanced ones. By having the &ldquo;multi_class&rdquo; parameter as &ldquo;multinomial&rdquo;(which uses the cross-entropy loss), I can apply it for my multiclass case. I also test it with different &ldquo;C&rdquo; (inverse of regularization strength) of values 0.01, 0.05, 0.25, 0.5, 1 to see which one gives the highest accuracy. Though this is a more simple model compare to the rest, so I expected the accuracy will not be as high.</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> [<span style=color:#ae81ff>0.01</span>, <span style=color:#ae81ff>0.05</span>, <span style=color:#ae81ff>0.25</span>, <span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>1</span>]:
    log_reg <span style=color:#f92672>=</span> LogisticRegression(C<span style=color:#f92672>=</span>x, solver<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;lbfgs&#39;</span>, multi_class<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;multinomial&#39;</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>fit(X_train, y_train)
    <span style=color:#66d9ef>print</span> (<span style=color:#e6db74>&#34;Logistics regression accuracy (tfidf) for &#34;</span>, x, <span style=color:#e6db74>&#34;:&#34;</span>, accuracy_score(y_train, log_reg<span style=color:#f92672>.</span>predict(X_train)))

log_reg <span style=color:#f92672>=</span> LogisticRegression(C<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, solver<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;lbfgs&#39;</span>, multi_class<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;multinomial&#39;</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>fit(X_train, y_train)
<span style=color:#66d9ef>print</span> (<span style=color:#e6db74>&#34;Logistics regression accuracy (tfidf) for C=1:&#34;</span>, accuracy_score(y_train, log_reg<span style=color:#f92672>.</span>predict(X_train)))
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>Out[<span style=color:#ae81ff>2</span>]: <span style=color:#ae81ff>0.9872372372372372</span>
</code></pre></div><ul><li>I apply the <em>Gaussian Naive Bayes Classifier</em>, a variant of Naive Bayes, for the next model comparison. Since this algorithm has a different approach in building up a simple model (by assuming data is described by a normal distribution), I am curious to see how this simple supervised learning algorithms perform compares to others.</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>naive_bayes <span style=color:#f92672>=</span> GaussianNB()<span style=color:#f92672>.</span>fit(X_train, y_train)
<span style=color:#66d9ef>print</span> (<span style=color:#e6db74>&#34;Naive Bayes accuracy: &#34;</span>, accuracy_score(y_train, naive_bayes<span style=color:#f92672>.</span>predict(X_train)))
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>Out[<span style=color:#ae81ff>4</span>]: <span style=color:#ae81ff>0.9204204204204204</span>
</code></pre></div><ul><li>Another supervised learning algorithm is the <em>random forest classifier</em>. But unlike the two algorithms above, it uses decision trees combination and voting mechanisms to perform its prediction. Because the <em>random forest classifier</em> is not biased and more stable when get fed with new data (opposite from Logistics Regression), I want to test out this algorithm after being concerned the algorithms above are both overfitted by producing such high accuracy scores (above 90%).</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>randomfor_reg <span style=color:#f92672>=</span> RandomForestClassifier(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>, max_depth<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>fit(X_train, y_train)
<span style=color:#66d9ef>print</span> (<span style=color:#e6db74>&#34;Random forest classifier accuracy: &#34;</span>, accuracy_score(y_train, randomfor_reg<span style=color:#f92672>.</span>predict(X_train)))
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>Out[<span style=color:#ae81ff>6</span>]: <span style=color:#ae81ff>0.5367867867867868</span>
</code></pre></div><ul><li>An algorithm that uses a similarity measure of data points to perform classification is <em>K-Nearest Neighbors Classification</em>. This is a good algorithm to use if the data is not linearly separable.</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>k_neighbor <span style=color:#f92672>=</span> KNeighborsClassifier(n_neighbors<span style=color:#f92672>=</span><span style=color:#ae81ff>15</span>)<span style=color:#f92672>.</span>fit(X_train, y_train)
<span style=color:#66d9ef>print</span> (<span style=color:#e6db74>&#34;K-Nearest neighbor accuracy: &#34;</span>, accuracy_score(y_train, k_neighbor<span style=color:#f92672>.</span>predict(X_train)))
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>Out[<span style=color:#ae81ff>8</span>]: <span style=color:#ae81ff>0.5535535535535535</span>
</code></pre></div><ul><li>Last but not least, I use the <em>Support vector machines (SVMs)</em> as the last model to perform classification and compare the result. Because SVMs have been reported to work better for text classification and it is very effective in high dimensional spaces, I expect this to return relatively higher accuracy compares to the rest.</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>supportvector <span style=color:#f92672>=</span> svm<span style=color:#f92672>.</span>SVC(decision_function_shape<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;ovo&#34;</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>fit(X_train, y_train)
<span style=color:#66d9ef>print</span> (<span style=color:#e6db74>&#34;SVM accuracy: &#34;</span>, accuracy_score(y_train, supportvector<span style=color:#f92672>.</span>predict(X_train)))
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>Out[<span style=color:#ae81ff>10</span>]: <span style=color:#ae81ff>0.9109109109109109</span>
</code></pre></div><h3 id=overall-model-performance>Overall Model Performance</h3><p>The accuracy scores of the training data set from the five models above surprise me. Specifically, the three models that give the highest scores are Logistic Regression (98.72%), Gaussian Naive Bayes Classifier (92.04%), and Support vector machines (91.09%). The two worst-performing models are the random forest classifier (53.68%) and K-Nearest Neighbors Classification (55.35%). It is also interesting to note there are two clusters of accuracy scores produced by the models above.</p><p>My initial thought was that the top performance models are overfitted. Although I did apply many crucial text pre-processing techniques, any accuracy scores that are above 90% when dealing with real-world data is a bit too high. I would not be concerned if the two most simple classifiers (Logistic and Gaussian Naive Bayes) would be overfitted, but SVMs should not be.</p><p>Moving on to the cluster of lower score models. Although the scores are a lot lower than the other cluster (at least 36% score difference), not only they are above the 50% mark, these models do not have the overfitting problem (hence the reason I included them for this project).</p><p>The next step is to implement these models to the test data set to see how well they can predict the ratings based on customers reviews:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>log_reg_test <span style=color:#f92672>=</span> log_reg<span style=color:#f92672>.</span>predict(X_test)
naive_bayes_test <span style=color:#f92672>=</span> naive_bayes<span style=color:#f92672>.</span>predict(X_test)
randomfor_reg_test <span style=color:#f92672>=</span> randomfor_reg<span style=color:#f92672>.</span>predict(X_test)
k_neighbor_test <span style=color:#f92672>=</span> k_neighbor<span style=color:#f92672>.</span>predict(X_test)
supportvector_test <span style=color:#f92672>=</span> supportvector<span style=color:#f92672>.</span>predict(X_test)

<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;Log regression: &#39;</span>, accuracy_score(y_test, log_reg_test))
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;Naive Bayes: &#39;</span>, accuracy_score(y_test, naive_bayes_test))
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;Random forest regression: &#39;</span>, accuracy_score(y_test, randomfor_reg_test))
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;K-nearest neighbor: &#39;</span>, accuracy_score(y_test, k_neighbor_test))
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;Support vector machines: &#39;</span>, accuracy_score(y_test, supportvector_test))
</code></pre></div><pre><code>Log regression:  0.6396396396396397
Naive Bayes:  0.4244244244244244
Random forest regression:  0.5375375375375375
K-nearest neighbor:  0.5545545545545546
Support vector machines:  0.6046046046046046
</code></pre><p>As expected, while there is not a major difference between test data accuracy scores and training data accuracy scores for the lower score models' cluster, the higher score models' cluster test data accuracy scores drop (might due to the model simplicity, Naive Bayes becomes the worst performing model). By being the best performing model in the train data, Log regression is also the best model in the test data with a 63.96% accuracy score.</p><p>In the figure below, I apply a confusion matrix to the multi-class classification model. Besides the chosen metric (accuracy score), there are three more to be considered: precision, recall, and f1-score. Precision answers the question: what is the proportion of all predicted positive being truly positive? Recall answers the question: what proportion of actual positives is correctly classified? In other words, precision emphasizes the false positive, and recall emphasizes the false negative. Because false positive or false negative outcome does not have much of a difference, let&rsquo;s look at the f1-score instead. F1-score is a good metric for my case because it is the balance between precision and recall. Figure 11 shows that the macro average of the f1-score is <em>0.41</em> and the weighted average of the f1-score is <em>0.57</em>.</p><p><img src=/posts/images/improve_product_review_system/fig11.png alt="Figure 11" class=center><div style=margin-top:rem></div></p><p>Lastly, I plot the area under the curve (AUC)- receiver operating characteristic (ROC) curve. This plot tells us, specifically, how well my model can distinguish the classes (ratings) by showing the trade-off of true positive rate and false-positive rate for different threshold settings of the underlying model. Generally, if the curve is above the diagonal line (chance level) and the area is above 0.5, it can be considered a good ROC curve.</p><p><img src=/posts/images/improve_product_review_system/fig12.png alt="Figure 12" class=center><div style=margin-top:rem></div></p><h2 id=conclusion>Conclusion</h2><p>Based on the accuracy score being almost 64%, it is safe to say that the Log regression model predicts the rating based on the customer&rsquo;s review quite accurately. Since I also consider other metrics, I do need to keep in mind that f1-scores are quite low (especially the macro average of the f1-score being below 0.5). Taking a look at the AUC-ROC curve, by having all AUC values above 0.75, proves that this model can also distinguish the different ratings very well. In conclusion, I have reached my goal for this project by not only making the model works but also make it produce a great result.</p><p>I believe that with more data from other ratings (since &ldquo;5&rdquo; stars rating takes a big proportion of the entire data set), the model&rsquo;s accuracy scores will be improved. Because I have a large enough sample size, I could have applied k-fold validation to eliminate the overfitting problem while predicting the train data set.</p><p>If I was to continue this project, I would apply some front-end works by productionize the model. I want to make it predicts the rating I want to give to a product based on the sentiment of my review text after I hit the submit (submit the review) button. After that, it will analyze all the reviews per rating and pick out the keywords of each rating. This helps the customer to see what are the most commonly discussed topic from each of the ratings. If I can apply this system to an online store, it will eliminate a step that requires a customer to write a review and help them to get a glance at what others are saying about the product before buying. Hence better customer experience and interaction.</p><p><img src=/posts/images/improve_product_review_system/fig13.png alt="Figure 13" class=center><div style=margin-top:rem></div></p><h2 id=code>Code</h2><p>This project&rsquo;s codes can be viewed at this GitHub&rsquo;s <a href=https://github.com/chilam27/Improve_Product_Review_System>repository</a>.</p><h2 id=update-061021><strong>Update!</strong> (06/10/21)</h2><p>I had successfully deployed the machine learning model using Heroku and Streamlit! Check out this website for the application: <a href=https://amazon-product-review-star.herokuapp.com/>https://amazon-product-review-star.herokuapp.com/</a>. Also, you can check out my <a href=https://github.com/chilam27/Improve_Review_Product_System_Deploy>&ldquo;Improve_Review_System_Deploy&rdquo;</a> repository for the code and data files.</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://player.vimeo.com/video/577437892 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 title="vimeo video" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><h2 id=author>Author</h2><ul><li><strong>Chi Lam</strong>, <em>student</em> at Michigan State University - <a href=https://github.com/chilam27>chilam27</a></li></ul><h2 id=acknowledgments>Acknowledgments</h2><p><a href=https://stackoverflow.com/a/38083189/138160970>Abhinav Arora. &ldquo;How to increase the model accuracy of logistic regression in Scikit python?&rdquo; #138160970. 28 June 2016. Forum post.</a></p><p><a href=https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/>Bansal, S. (2016, August 24). Beginners Guide to Topic Modeling in Python and Feature Selection.</a></p><p><a href=https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/>BhandariI, A. (2020, April 03). Feature Scaling: Standardization Vs Normalization.</a></p><p><a href=https://towardsdatascience.com/sentiment-analysis-with-python-part-2-4f71e7bde59a>Kub, A. (2019, January 24). Sentiment Analysis with Python (Part 2).</a></p><p><a href=https://towardsdatascience.com/roc-curve-explained-using-a-covid-19-hypothetical-example-binary-multi-class-classification-bab188ea869c>Loukas, S. (2020, June 14). ROC Curve Explained using a COVID-19 hypothetical example: Binary & Multi-Class Classification&mldr;</a></p><p><a href=https://www.kaggle.com/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews/notebook>N, L. (2019, June 19). Sentiment Analysis of IMDB Movie Reviews.</a></p><p><a href=https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62>Narkhede, Sarang. (2018, May 9). Understanding Confusion Matrix.</a></p><p><a href=https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/>Rai, A. (2019, January 23). Python: Sentiment Analysis using VADER.</a></p><p><a href=https://stackoverflow.com/a/47091490/13816097>Yann Dubois. &ldquo;Expanding English language contractions in Python&rdquo; #13816097. 03 November 2017. Forum post.</a></p></div><div class=btn-improve-page><a href=https://github.com/hugo-toha/hugo-toha.github.io/edit//content/posts/improve_product_review_system.md title="Improve this page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>Improve this page</a></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/analyze_predict_house_price/analyze_predict_house_price/ title="Analyze and Predict House Prices" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i>Prev</div><div class=next-prev-text>Analyze and Predict House Prices</div></a></div><div class="col-md-6 next-article"><a href=/posts/pubg_player_role_analysis/ title="PUBG Player Role Analysis" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>PUBG Player Role Analysis</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#background-and-motivation>Background and Motivation</a></li><li><a href=#prerequisites>Prerequisites</a></li><li><a href=#project-outline>Project Outline</a><ul><li><a href=#data-collection>Data Collection</a></li><li><a href=#text-preprocessing>Text Preprocessing</a></li><li><a href=#eda>EDA</a></li><li><a href=#model-building>Model Building</a></li><li><a href=#overall-model-performance>Overall Model Performance</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#code>Code</a></li><li><a href=#update-061021><strong>Update!</strong> (06/10/21)</a></li><li><a href=#author>Author</a></li><li><a href=#acknowledgments>Acknowledgments</a></li></ul></nav></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=/#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=/#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=/#education>Education</a></li><li class=nav-item><a class=smooth-scroll href=/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span>Email:</span> <span>lamdaochi27@gmail.com</span></li><li><span>Phone:</span> <span>(+1) 857-310-9338</span></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>