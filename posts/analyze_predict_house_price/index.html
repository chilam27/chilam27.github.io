<!doctype html><html><head><title>Analyze and Predict House Prices</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><meta property="og:title" content="Analyze and Predict House Prices"><meta property="og:description" content="Analyze and predict Boston housing prices through Trulia data using regression machine learning models."><meta property="og:type" content="article"><meta property="og:url" content="https://hugo-toha.github.io/posts/analyze_predict_house_price/"><meta property="article:published_time" content="2020-07-13T00:00:00+00:00"><meta property="article:modified_time" content="2020-07-13T00:00:00+00:00"><meta name=description content="Analyze and predict Boston housing prices through Trulia data using regression machine learning models."><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/>Dao Chi's Portfolio</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><a class=active href=/posts/analyze_predict_house_price/ title="Analyze and Predict House Prices">Analyze and Predict House Prices</a></li><li><a href=/posts/improve_product_review_system/ title="Improve Product Review System">Improve Product Review System</a></li><li><a href=/posts/pubg_player_role_analysis/ title="PUBG Player Role Analysis">PUBG Player Role Analysis</a></li><li><a href=/posts/stock_price_prediction/ title="Stock Price Prediction">Stock Price Prediction</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://hugo-toha.github.io/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/default-avatar_hu64ccddcebcea17e48c2247350ae0b3d2_162491_120x120_fit_q75_box.jpg alt="Author Image"><h5 class=author-name>Dao Chi Lam</h5><p>July 13, 2020</p></div><div class=title><h1>Analyze and Predict House Prices</h1></div><div class=post-content id=post-content><p>Analyzing and predict Boston housing prices using data scraped from trulia.com using advanced regression models. I will go in-depth for all the processes of this project: using <code>Beautiful Soup</code> to scrape data, analyzing and comparing different regression models, and building an Application Programming Interface (API) using <code>Flask</code>.</p><h2 id=background-and-motivation>Background and Motivation</h2><p>My goal for this project is to use the best regression model to predict Boston housing prices on Trulia based on the number of features that the source provides. By the end of the project, I am hoping to be able to answer these questions:</p><ul><li>What are the features that have high effects on the rent of a property? Does the <em>neighborhood</em> of the property have any effect?</li><li>How good is our prediction for the rent of the property?</li></ul><p>Since I have done a simple regression model with my previous project (<a href=https://github.com/chilam27/GDP_Fertility_Mortality_Relation/edit/master/README.md>&ldquo;GDP_Fertility_Mortality_Relation&rdquo;</a>), I want to improve my skill sets and prediction accuracy even more. In this project, I have three main focuses:</p><ol><li>Learn how to scrape a website with Python.</li><li>Understand the importance of Exploratory Data Analysis and how it contributes to the outcome.</li><li>Introduce the productionization stage into my project.</li></ol><p>One more improvement I apply in this project is using different regression models and improve its performance by fine-tuning the parameters using <code>GridSearchCV</code>.</p><h2 id=prerequisites>Prerequisites</h2><p>Python Version: 3.7.4</p><p>Packages: BeautifulSoup, json, urllib, pandas, numpy, sklearn, matplotlib, seaborn, scipy, statsmodels, xgboost, pickle, flask.</p><p>Web Framework Requirements: <code>pip install -r requirements.txt</code></p><h2 id=project-outline>Project Outline</h2><ol><li><p>Data collection: use <code>BeautifulSoup</code> to scrape property data from Trulia, a popular real estate website. Gather all listed variables that can be used for data analysis.</p></li><li><p>Data cleaning: read in data and prepare it for data analysis; steps include: tidy up the categorical features, deal with null values, etc.</p></li><li><p>Exploratory data analysis (EDA): examine the cleaned data and its trends so we can choose an appropriate model that can be applied.</p></li><li><p>Model building: determine which model (<code>Linear</code>, <code>Lasso</code>, <code>Random Forest</code>, <code>XGBoost</code>) works best (that returns the smallest error) and tune the model with different parameters using <code>GridSearchCV</code>.</p></li><li><p>Productioniize model: create a <em>local</em> API to get quick access to the regression model with a given input set.</p></li></ol><p>To evaluate the performance of our model, I will use the mean absolute error (MAE) as the metric for this project. The reason for choosing this metric is because it can represent, on average, how far off our prediction is.</p><h3 id=data-collection>Data Collection</h3><p><em>*Disclaimer: this data set is used for educational purposes.</em></p><p>I want to give the acknowledgment of this scrapping code to Karishma Parashar (the GitHub repository for her code can be found <a href=https://github.com/Abmun/WebScraping-RentalProperties>here</a>). Her code gives a nice outline for the process. Though there are some bugs that I needed to fix for the code to run properly and to get the data and the amount I needed.</p><p>I started the scrapping procedure on June 25h, 2020. My goal is to scrape, at most, 240 records (not all search terms will result with at least 240 records) from each neighborhood of Boston and convert it to a CSV file. I have collected 3,894 records and 12 different variables (<a href=https://github.com/chilam27/Boston_Housing_Prices/blob/master/housing_data_scraped.csv>&ldquo;housing_data_scraped.csv&rdquo;</a>).</p><p>One thing to note is the &lsquo;feature&rsquo; variable: I could not code the scrapping process to attain all features because of the &lsquo;See All" button that hides some of the data. Do not rely heavily on this variable when doing analysis.</p><p>Neighborhoods as search key terms: East Boston, Charlestown, Allston, North End, West End, Downtown, Chinatown, Back Bay/ Beacon Hill, South Boston, South End, Fenway, Mission Hill, Roxbury, Dorchester, Jamaica Plain, Mattapan, Roslindale, West Roxbury, Hyde Park.</p><table><thead><tr><th style=text-align:center>Variables</th><th style=text-align:center>Description</th></tr></thead><tbody><tr><td style=text-align:center>rent</td><td style=text-align:center>the rent of the property for 1 month</td></tr><tr><td style=text-align:center>address</td><td style=text-align:center>the address of the property</td></tr><tr><td style=text-align:center>area</td><td style=text-align:center>neighborhood the property located</td></tr><tr><td style=text-align:center>bed</td><td style=text-align:center>number of beds provided</td></tr><tr><td style=text-align:center>bath</td><td style=text-align:center>number of bathrooms provided</td></tr><tr><td style=text-align:center>school</td><td style=text-align:center>number of schools around the area</td></tr><tr><td style=text-align:center>crime</td><td style=text-align:center>crime rate of the area</td></tr><tr><td style=text-align:center>commute</td><td style=text-align:center>percentage of people commute by car</td></tr><tr><td style=text-align:center>shop_eat</td><td style=text-align:center>number of shops and restaurants in the area</td></tr><tr><td style=text-align:center>description</td><td style=text-align:center>description of the property</td></tr><tr><td style=text-align:center>feature</td><td style=text-align:center>item that property provides (heating, laundry, etc.)</td></tr><tr><td style=text-align:center>URL</td><td style=text-align:center>link to the property</td></tr></tbody></table><h3 id=data-cleaning>Data Cleaning</h3><ul><li><p>Check for null value and remove duplicate.</p></li><li><p>Clean up the text for each column.</p></li><li><p>&lsquo;area&rsquo; column: rename North End, Downtown, Chinatown, and West End to Central; rename South Dorchester and North Dorchester as Dorchester; rename Beacon Hill and Back Bay as Back Bay/ Beacon Hill.</p></li><li><p>&lsquo;school&rsquo; column: split values in column accordingly and add value to these new columns: &lsquo;elementary_school&rsquo;, &lsquo;middle_school&rsquo;, and &lsquo;high_school&rsquo;; delete &lsquo;school&rsquo; column.</p></li><li><p>&lsquo;shop_eat&rsquo; column: split values in column accordingly and add value to these new columns: &lsquo;restaurant&rsquo;, &lsquo;grocery&rsquo;, and &lsquo;nightlife&rsquo;; delete &lsquo;shop_eat&rsquo; column.</p></li><li><p>&lsquo;feature&rsquo; column: split values in column accordingly and add value to these new columns: &lsquo;property_type&rsquo;, &lsquo;pet_allowed&rsquo;, &lsquo;laundry&rsquo;, &lsquo;parking&rsquo;, &lsquo;ac&rsquo;, &lsquo;dishwasher&rsquo;, &lsquo;washer&rsquo;, &lsquo;dryer&rsquo;, &lsquo;fridge&rsquo;, and &lsquo;total_amenties&rsquo;; delete &lsquo;feature&rsquo; column.</p></li><li><p>Change data type accordingly to the variable and reorder columns</p></li><li><p>Deleting unnecessary columns: &lsquo;description&rsquo;, &lsquo;address&rsquo;, &lsquo;url&rsquo;.</p></li></ul><p>Below is an image of what the data frame looks like:</p><p><img src=/posts/images/analyze_predict_house_price/fig1.png alt="Figure 1" class=center><div style=margin-top:rem></div></p><h3 id=eda>EDA</h3><ul><li>Here is the general description of our variables: we have no missing value or any null value and all the data types are in place.</li></ul><p><img src=/posts/images/analyze_predict_house_price/fig2.png alt="Figure 2" class=center><div style=margin-top:rem></div></p><ul><li>Univariate analysis on target variable (&lsquo;rent&rsquo;): calculate the skewness and kurtosis of the variable; plot the value and examine if the distribution shape is normal. From the table and graph, we can see that: since the original data has high positive skewness and kurtosis (the curve is formed by a huge cluster of mid-range properties and few expensive properties that cause it to have a right skew), I normalize the data by performing log transformation and it resulted very close to a normal distribution.</li></ul><p><img src=/posts/images/analyze_predict_house_price/fig3.png alt="Figure 3" class=center>
<img src=/posts/images/analyze_predict_house_price/fig4.png alt="Figure 4" class=center>
<img src=/posts/images/analyze_predict_house_price/fig5.png alt="Figure 5" class=center><div style=margin-top:rem></div></p><ul><li>Multivariate analysis on target variable: have the bathroom as an additional dependent variable and see the relationship between rent and number of bathrooms a property has. With the plot below, there is an upward trend.</li></ul><p><img src=/posts/images/analyze_predict_house_price/fig6.png alt="Figure 6" class=center>
<img src=/posts/images/analyze_predict_house_price/fig7.png alt="Figure 7" class=center><div style=margin-top:rem></div></p><ul><li>Remove outliers: I use Z-score to help me identify and remove outliers from the data frame. This has improved the distribution by a great amount.</li></ul><p><img src=/posts/images/analyze_predict_house_price/fig8.png alt="Figure 8" class=center><div style=margin-top:rem></div></p><ul><li><p>Determine the numerical and categorical variables</p><ul><li>Numerical: &lsquo;rent&rsquo;, &lsquo;bed&rsquo;, &lsquo;bath&rsquo;, &lsquo;school&rsquo;, &lsquo;elemenatary_school&rsquo;, &lsquo;middle_school&rsquo;, &lsquo;high_school&rsquo;, &lsquo;car_commute_percent&rsquo;, &lsquo;total_amenties&rsquo;, &lsquo;laundry&rsquo;, &lsquo;ac&rsquo;, &lsquo;dishwasher&rsquo;, &lsquo;washer&rsquo;, &lsquo;dryer&rsquo;, &lsquo;fridge&rsquo;, &lsquo;pet_allowed&rsquo;, &lsquo;parking&rsquo;, &lsquo;restaurant&rsquo;, &lsquo;grocery&rsquo;, &lsquo;nightlife&rsquo;.</li><li>Categorical: &lsquo;area&rsquo;, &lsquo;property_type&rsquo;, &lsquo;crime&rsquo;.</li></ul></li><li><p>Observe to see how variables are correlated to each other through heatmap: from this plot, there are many interesting details that we need to pay attention to.</p><ul><li><p>&lsquo;bed&rsquo;, &lsquo;bath&rsquo;, &lsquo;restaurant&rsquo;, &lsquo;grocery&rsquo;, and &lsquo;nightlife&rsquo;, accordingly, have the highest correlation with the target variable. Though they are less correlated than expected.</p></li><li><p>All the school variables are highly correlated to each other. This is predicted since the values were derived from a single variable during our cleaning phase. Similar can be said with &lsquo;restaurant&rsquo;, &lsquo;grocery&rsquo;, and &lsquo;nightlife&rsquo; (they came from a variable that is called &lsquo;shop_eat&rsquo;)</p></li><li><p>All the features variables interesting have a lower correlation with the target variable. Keep in mind that we will not use much of this data.</p></li></ul></li></ul><p><img src=/posts/images/fig9.png alt="Figure 9" class=center><div style=margin-top:rem></div></p><ul><li>Observed only those numerical variables that have a high correlation with the target variable with zoomed heatmap. Since we have mentioned that &lsquo;restaurant&rsquo;, &lsquo;grocery&rsquo;, and &lsquo;nightlife&rsquo; are pretty similar, we can remove two of the variables and keep &lsquo;grocery&rsquo; for now for analysis (figure 12).</li></ul><p><img src=/posts/images/analyze_predict_house_price/fig10.png alt="Figure 10" class=center>
<img src=/posts/images/analyze_predict_house_price/fig11.png alt="Figure 11" class=center>
<img src=/posts/images/analyze_predict_house_price/fig12.png alt="Figure 12" class=center><div style=margin-top:rem></div></p><ul><li>Next is to examine the categorical variable. I explore the &lsquo;area&rsquo; variable with the assumption that: depends on the neighborhood, the general trend for the rent of a property might be different. Here, I make a pie chart shows the proportion of area that each neighborhood takes up and a boxplot of the relationship between &lsquo;area&rsquo; and &lsquo;rent&rsquo;. By looking at the mean and its ranges, there seems to be some correlation. Another thing worth noting is the number of outliers presented in almost every neighborhood.</li></ul><p><img src=/posts/images/analyze_predict_house_price/fig13.png alt="Figure 13" class=center>
<img src=/posts/images/analyze_predict_house_price/fig14.png alt="Figure 14" class=center>
<img src=/posts/images/analyze_predict_house_price/fig15.png alt="Figure 15" class=center><div style=margin-top:rem></div></p><ul><li>Examine &lsquo;property_type&rsquo; and &lsquo;crime&rsquo; and their relation with rent:<ul><li>Because there is an inbalance proportion of data for &lsquo;property_type&rsquo;(appartment 3%, condo >0.1%, multi 96%, single 1%, townhouse and condo are > 1%), it can be hard to examin the relationship accurately.</li><li>Based on the graph, we can see a representation of what it would look like: multi-family and townhouse have about the same median, a little bit higher than that is single-family; although apartment seems to have the same median as a condo, I believe if we have enough data points, the condo would be the property with the highest rent.</li><li>As for &lsquo;crime&rsquo;, Moderate and Low crime rates have the same median and range. Although the &ldquo;High&rdquo; rate does have a similar median but interestingly has a higher range.</li><li>The two most surprising ones are the Lowest and Highest rates: while the &ldquo;Lowest&rdquo; one also has the lowest median and, arguably, range, the Highest rate has the highest median. I expected it to be the opposite for these two. Based on this, maybe properties that are more expensive attract more crime than lower ones?</li></ul></li></ul><p><img src=/posts/images/analyze_predict_house_price/fig16_17.png alt="Figure 16 & 17" class=center><div style=margin-top:rem></div></p><ul><li>Create dummies variable for the categorical variables: &lsquo;area&rsquo;, &lsquo;property_type&rsquo;, &lsquo;crime&rsquo;</li></ul><h3 id=regression-model>Regression Model</h3><ul><li><p>Split the data to training (80%) and testing (20%) sets in a stratify fashion: stratas are the different neighborhoods. The reason for this is because I want to have the coverage of the entire area in Boston city and some neighborhood have significantly lower data than others.</p></li><li><p>Testing assumptions - Normality/ Linearity: when I was investigating our rent variable during the EDA stage, I have transformed the data into log so it can prevent underfitting. But I tested it again for our train data to be sure also.</p></li></ul><p><img src=/posts/images/analyze_predict_house_price/fig18.png alt="Figure 18" class=center>
<img src=/posts/images/analyze_predict_house_price/fig19.png alt="Figure 19" class=center><div style=margin-top:rem></div></p><ul><li><p>Testing assumptions - Homoscedasticity: I included all variables to eliminate heteroscedasticity.</p></li><li><p>Testing assumptions - Absence of Multicollinearity: though I have tried to remove variables that are highly correlated to each other, I found in the result of regression models that by including every variable I was able to get a higher accuracy rate.</p></li><li><p>Ordinary least squares (OLS) regression: by being one of the simplest estimators for a simple model, it is easier to implement and interpret compare to other sophisticated ones. Although it might not work well with my project that has many variables and correlations to the target is week, I gave it a shot. Besides the table of correlation that I already have a general picture of what it is like, a statistical measurement that I am interested in is the &ldquo;R-squared&rdquo; (coefficient of determination) of <strong>0.661</strong>. This means that our model fits well and we can predict about 66.1% of our trained data set. Keep in mind that although the difference between &ldquo;R-squared&rdquo; and &ldquo;Adj. R-squared&rdquo; (<strong>0.657</strong>) is not too significant, it let us know that there are some irrelevant features that we have included in our model. Another interesting measurement is &ldquo;F-statistics&rdquo; which has a value above 100 (<em>152.3</em>) and &ldquo;Prop (F-statistics)&rdquo; that has a value below 0.05 (<em>0.00</em>). By having these two measurements meeting the condition, it means that there is a good linear relationship between the target and all the feature variables.</p></li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>X_sm <span style=color:#f92672>=</span> X <span style=color:#f92672>=</span> sm<span style=color:#f92672>.</span>add_constant(X) <span style=color:#75715e>#create a column of all &#39;1&#39; create an intercept to the slope of the regression line; this is necessary for stats model</span>
<span style=color:#66d9ef>del</span> X_sm[<span style=color:#e6db74>&#39;area&#39;</span>]
model <span style=color:#f92672>=</span> sm<span style=color:#f92672>.</span>OLS(np<span style=color:#f92672>.</span>asarray(y), X_sm<span style=color:#f92672>.</span>astype(float))
model<span style=color:#f92672>.</span>fit()<span style=color:#f92672>.</span>summary()
</code></pre></div><p><img src=/posts/images/analyze_predict_house_price/fig20.png alt="Figure 20" class=center><div style=margin-top:rem></div></p><p><em>Throughout all of the model, I will implement cross-validation to eliminate the probability that the model might be overfitting or contains bias</em></p><ul><li>Multiple linear regression: (this is the same model of what we have discussed above, but I use mean absolute error here as my measurement of the model accuracy)</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>reg_lin <span style=color:#f92672>=</span> LinearRegression()
reg_lin<span style=color:#f92672>.</span>fit(X_train, y_train)
np<span style=color:#f92672>.</span>mean(cross_val_score(reg_lin, X_train, y_train, scoring <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;neg_mean_absolute_error&#39;</span>))
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>Out[<span style=color:#ae81ff>1</span>]: <span style=color:#f92672>-</span><span style=color:#ae81ff>393.7313</span>
</code></pre></div><ul><li>Lasso (least absolute shrinkage and selection operator) regression: in contrast with the model above, I tested out the Lasso regression model because of its ability to analyze data set with large features very well. That is not the only reason for applying in this model because I want to see how its L1 regularization technique works (helps with eliminating overfitting). For this model, I have tested out a range of alpha from 1 to 100 with an increment of 10. By finding the maximum error of the curve of the plot below, we have alpha = 1.7.</li></ul><p><img src=/posts/images/analyze_predict_house_price/fig21.png alt="Figure 21" class=center><div style=margin-top:rem></div></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>reg_las <span style=color:#f92672>=</span> Lasso(alpha <span style=color:#f92672>=</span> alpha[y_max_index])
reg_las<span style=color:#f92672>.</span>fit(X_train, y_train)
np<span style=color:#f92672>.</span>mean(cross_val_score(reg_las, X_train, y_train, scoring <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;neg_mean_absolute_error&#39;</span>))
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>Out[<span style=color:#ae81ff>2</span>]: <span style=color:#f92672>-</span><span style=color:#ae81ff>392.8202</span>
</code></pre></div><ul><li>Random forest regression: this model apply some different techniques to predict the data: it uses multiple decision trees and bagging (or bootstrap aggregation) to better understand the bias and variance of the data. Because it also works well with large data set, I applied this model to our data to see how it does compare to other models.</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>reg_rf <span style=color:#f92672>=</span> RandomForestRegressor(random_state <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>)
reg_rf<span style=color:#f92672>.</span>fit(X_train, y_train)
np<span style=color:#f92672>.</span>mean(cross_val_score(reg_rf, X_train, y_train, scoring <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;neg_mean_absolute_error&#39;</span>))
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>Out[<span style=color:#ae81ff>3</span>]: <span style=color:#f92672>-</span><span style=color:#ae81ff>306.6928</span>
</code></pre></div><ul><li>XGBoost: being known for its ability to outperform any of its competitors, XGBoost is my last algorithm of choice to predict our data. Different from random forest regression, it uses boosting technique (combining weak learners to strong ones to improve prediction accuracy). It is also powerful for the wide range of parameters to really fine-tune the algorithm.</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>reg_xgboost <span style=color:#f92672>=</span> xgb<span style=color:#f92672>.</span>XGBRegressor()
reg_xgboost<span style=color:#f92672>.</span>fit(X_train, y_train)
np<span style=color:#f92672>.</span>mean(cross_val_score(reg_xgboost, X_train, y_train, scoring <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;neg_mean_absolute_error&#39;</span>))
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>Out[<span style=color:#ae81ff>4</span>]: <span style=color:#f92672>-</span><span style=color:#ae81ff>343.4491</span>
</code></pre></div><h3 id=overall-model-performance>Overall Model Performance</h3><p>Before finalizing our model, I made three different adjustments to the data to see if it can improve the prediction:</p><ol><li>Size: initially, after finished cleaning the data, I trimmed the data set from 3000+ records to about 759 records. I trimmed it based on the proportion of the area that each neighborhood takes relative to the area of the city. The reason why the size is small is that for a neighborhood like Hyde Park and Mattapan has very few records on Trulia. But it turned out that it does not give a very good prediction as if I leave the size as it is. This concluded that the larger the sample size is the better the prediction is.</li><li>Stratification: in order for our sample to accurately represent the city of Boston, I make sure all neighborhoods are present in the training data set. As it turned out, this also resulted in a better prediction for our model.</li><li>Eliminate irrelevant features: recalling back to our heat maps from above, there are only a few variables that give me the correlation with the target variable that is above 0.2. I thought, by removing the irrelevant features and those that are highly correlated to each other, would increase the performance. But it did not turn about to be like that. I am still unsure of why this is the case. But by including all variables, it seems to perform better (the larger the number of features is the better?)</li></ol><p>There are two things in the list of the outcome of our prediction models that surprised me: there is only <em>0.9</em> improvement in lasso regression compare to the multiple linear regression and the random forest regression outperformed the XGBoost algorithm by <em>36.7</em>. Because of this, I will apply an exhaustive <code>GridSearchCV</code> to search for the best parameters for the random forest regression. Although I have tried to exhausted tuning with many more parameters such as &ldquo;max_depth&rdquo; and &ldquo;min_samples_leaf&rdquo;, it took too much time so I did a simple one instead (so the difference of the result is not huge).</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>parameters <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;n_estimators&#39;</span>: [<span style=color:#ae81ff>200</span>, <span style=color:#ae81ff>400</span>, <span style=color:#ae81ff>600</span>, <span style=color:#ae81ff>800</span>],
              <span style=color:#e6db74>&#39;criterion&#39;</span>: [<span style=color:#e6db74>&#39;mse&#39;</span>, <span style=color:#e6db74>&#39;mae&#39;</span>], 
              <span style=color:#e6db74>&#39;max_features&#39;</span>: [<span style=color:#e6db74>&#39;auto&#39;</span>,<span style=color:#e6db74>&#39;sqrt&#39;</span>,<span style=color:#e6db74>&#39;log2&#39;</span>]}
grid <span style=color:#f92672>=</span> GridSearchCV(reg_rf, parameters, scoring <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;neg_mean_absolute_error&#39;</span>, cv<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>)
grid<span style=color:#f92672>.</span>fit(X_train, y_train)

grid<span style=color:#f92672>.</span>best_score_
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>Out[<span style=color:#ae81ff>5</span>]: <span style=color:#f92672>-</span><span style=color:#ae81ff>304.3920688932027</span>
</code></pre></div><p>With the best parameters for our best performance model through the training data set, now it is the time to implement it to our test data set too see how well our model can predict the prices of property in Boston:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>reg_lin_test <span style=color:#f92672>=</span> reg_lin<span style=color:#f92672>.</span>predict(X_test)  
reg_las_test <span style=color:#f92672>=</span> reg_las<span style=color:#f92672>.</span>predict(X_test)
reg_xgboost_test <span style=color:#f92672>=</span> reg_xgboost<span style=color:#f92672>.</span>predict(X_test)
reg_rf_test <span style=color:#f92672>=</span> grid<span style=color:#f92672>.</span>best_estimator_<span style=color:#f92672>.</span>predict(X_test)

<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;Multiple linear regression: &#39;</span>, mean_absolute_error(y_test, reg_lin_test))
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;Lasso regression: &#39;</span>, mean_absolute_error(y_test, reg_las_test))
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;XGBoost regression: &#39;</span>, mean_absolute_error(y_test, reg_xgboost_test))
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;Random forest regression regression (using best parameters through GridSearchCV): &#39;</span>, mean_absolute_error(y_test, reg_rf_test)) <span style=color:#75715e>#best one</span>
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>Multiple linear regression:  <span style=color:#ae81ff>1469.1753603498644</span>
Lasso regression:  <span style=color:#ae81ff>393.3840532780861</span>
XGBoost regression:  <span style=color:#ae81ff>403.93248876337</span>
Random forest regression regression (using best parameters through GridSearchCV):  <span style=color:#ae81ff>316.51083201892743</span>
</code></pre></div><h3 id=productionization>Productionization</h3><p>For this part of the project, I followed Chris I. article &ldquo;Productionize a Machine Learning model with Flask and Heroku&rdquo; and applied it for my project.</p><ul><li>Pickle the model to save the model by turning it to a byte stream</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>pickl <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;model&#39;</span>: grid<span style=color:#f92672>.</span>best_estimator_}
pickle<span style=color:#f92672>.</span>dump(pickl, open( <span style=color:#e6db74>&#39;model.pkl&#39;</span>, <span style=color:#e6db74>&#34;wb&#34;</span>)) 
</code></pre></div><ul><li><p>Creating a new python file with a list of ordered input values to act as input for the model.</p></li><li><p>Build a Flask (a micro web framework) API using Flask module by installing the requirements, create new python files, and import the necessary commands for those files (app.py, wsgi.py, requests.py, etc.)</p></li><li><p>Apply the input to the model with the working API.</p></li></ul><p><img src=/posts/images/analyze_predict_house_price/fig22.png alt="IDE Screenshot" class=center><div style=margin-top:rem></div></p><h2 id=conclusion>Conclusion</h2><p>With our best prediction model returns the MAE of 316.51 for the test data set, the model does predict the property&rsquo;s price according to the features that are used quite accurately. The MAE value of 316.51 means that on average, our prediction is off around 316.51. That is acceptable considering how low our correlation values are. We concluded that a fine tune random forest regression works the best in predicting property&rsquo;s rent based on 19 of the used features.</p><p>Although my first intention was to follow the tutorial by Chris I. and productionize the model into a public API with Flask and Heroku, because I could not spend more time tackling problems I had with Heroku so I made it local instead (I have attached my progress and the problem that I was on in figures below).</p><p><img src=/posts/images/analyze_predict_house_price/fig23.png alt="Production Screenshot" class=center>
<img src=/posts/images/analyze_predict_house_price/fig24.png alt="Production Screenshot" class=center><div style=margin-top:rem></div></p><h2 id=author>Author</h2><ul><li><strong>Chi Lam</strong>, <em>student</em> at Michigan State University - <a href=https://github.com/chilam27>chilam27</a></li></ul><h2 id=acknowledgments>Acknowledgments</h2><p><a href=medium.com/@kvnamipara/a-better-visualisation-of-pie-charts-by-matplotlib-935b7667d77f>Amipara, Kevin. Better Visualization of Pie Charts by MatPlotLib. 20 Nov. 2019.</a></p><p><a href=www.thoughtco.com/stratified-sampling-3026731>Crossman, Ashley. Understanding Stratified Samples and How to Make Them. 27 Jan. 2020.</a></p><p><a href=www.barrfoundation.org/blog/public-art-in-boston-neighborhoods>E. San San Wong, Cathy Edwards. Public Art Is Alive and Well in Boston Neighborhoods.</a></p><p><a href=towardsdatascience.com/productionize-a-machine-learning-model-with-flask-and-heroku-8201260503d2>I., Chris. Productionize a Machine Learning Model with Flask and Heroku. 9 Dec. 2019</a></p><p><a href=blog.usejournal.com/creating-an-unbiased-test-set-for-your-model-using-stratified-sampling-technique-672b778022d5>Irekponor, Victor E. CREATING AN UNBIASED TEST-SET FOR YOUR MODEL USING STRATIFIED SAMPLING TECHNIQUE. 14 Nov. 2019</a></p><p><a href=www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python>Marcelino, Pedro. Comprehensive Data Exploration with Python. 23 Aug. 2019.</a></p><p><a href=www.sciencebuddies.org/science-fair-projects/references/sample-size-surveys>Science Buddies. Sample Size: How Many Survey Participants Do I Need? 10 Aug. 2017.</a></p><p><a href=https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba>Sharma, Natasha. Ways to Detect and Remove the Outliers. 23 May 2018.</a></p></div><div class=btn-improve-page><a href=https://github.com/hugo-toha/hugo-toha.github.io/edit//content/posts/analyze_predict_house_price.md title="Improve this page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>Improve this page</a></div><hr><div class="row next-prev-navigator"><div class="col-md-12 next-article"><a href=/posts/improve_product_review_system/ title="Improve Product Review System" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>Improve Product Review System</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#background-and-motivation>Background and Motivation</a></li><li><a href=#prerequisites>Prerequisites</a></li><li><a href=#project-outline>Project Outline</a><ul><li><a href=#data-collection>Data Collection</a></li><li><a href=#data-cleaning>Data Cleaning</a></li><li><a href=#eda>EDA</a></li><li><a href=#regression-model>Regression Model</a></li><li><a href=#overall-model-performance>Overall Model Performance</a></li><li><a href=#productionization>Productionization</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#author>Author</a></li><li><a href=#acknowledgments>Acknowledgments</a></li></ul></nav></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=/#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=/#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=/#education>Education</a></li><li class=nav-item><a class=smooth-scroll href=/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span>Email:</span> <span>lamdaochi27@gmail.com</span></li><li><span>Phone:</span> <span>(+1) 857-310-9338</span></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">Â© 2020 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>